{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning entity recognition\n",
    "\n",
    "\n",
    "Objetive:Recognize a set of “Machine learning” entities from the analysis of ML text\n",
    "\n",
    "Examples of entities could be:\n",
    "\n",
    "    • Supervised ML algorithm (decision tree, random forest, SVM, etc.)\n",
    "    • Unsupervised ML algorithm (clustering, knn, event detection)\n",
    "    • ML software (keras, sklearn, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this approach we will\n",
    "    \n",
    "    •Train a model using a data set of IOB formatted sentences found in Kaggle\n",
    "    •Build a custom data set using machine learning texts\n",
    "    •Train our model using this custom data\n",
    "    •Train a spacy model using our custom data set\n",
    "    •Compare our model whit the spacy model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data set\n",
    "\n",
    "\n",
    "We procced to import the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1           NaN             of   IN      O\n",
       "2           NaN  demonstrators  NNS      O\n",
       "3           NaN           have  VBP      O\n",
       "4           NaN        marched  VBN      O\n",
       "5           NaN        through   IN      O\n",
       "6           NaN         London  NNP  B-geo\n",
       "7           NaN             to   TO      O\n",
       "8           NaN        protest   VB      O\n",
       "9           NaN            the   DT      O\n",
       "10          NaN            war   NN      O\n",
       "11          NaN             in   IN      O\n",
       "12          NaN           Iraq  NNP  B-geo\n",
       "13          NaN            and   CC      O\n",
       "14          NaN         demand   VB      O\n",
       "15          NaN            the   DT      O\n",
       "16          NaN     withdrawal   NN      O\n",
       "17          NaN             of   IN      O\n",
       "18          NaN        British   JJ  B-gpe\n",
       "19          NaN         troops  NNS      O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set is conformed by 4 columns \n",
    "\n",
    "    • Sentence #: indicates the sentence number each new sentence is indicated by a new sentence number\n",
    "    • Word: the word of the sentence\n",
    "    • POS: the part of speech tag for each word\n",
    "    • Tag: the entity classification according to IOB format  when I prefix indicates the word is inside of a chunk , B indicates the word is in the beggining of a chunk and O that the word is outside of a chunk (non classiication/ non entity)\n",
    "        geo = Geographical Entity\n",
    "        org = Organization\n",
    "        per = Person\n",
    "        gpe = Geopolitical Entity\n",
    "        tim = Time indicator\n",
    "        art = Artifact\n",
    "        eve = Event\n",
    "        nat = Natural Phenomenon\n",
    "        \n",
    "After, we will filter the model's prediction in order to obtain only the ML entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we can see there is plenty missing values in the dataset, it will be better for the classifier if we fill the missing values (sentece number) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.fillna(method='ffill')[:150000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1   Sentence: 1             of   IN      O\n",
       "2   Sentence: 1  demonstrators  NNS      O\n",
       "3   Sentence: 1           have  VBP      O\n",
       "4   Sentence: 1        marched  VBN      O\n",
       "5   Sentence: 1        through   IN      O\n",
       "6   Sentence: 1         London  NNP  B-geo\n",
       "7   Sentence: 1             to   TO      O\n",
       "8   Sentence: 1        protest   VB      O\n",
       "9   Sentence: 1            the   DT      O\n",
       "10  Sentence: 1            war   NN      O\n",
       "11  Sentence: 1             in   IN      O\n",
       "12  Sentence: 1           Iraq  NNP  B-geo\n",
       "13  Sentence: 1            and   CC      O\n",
       "14  Sentence: 1         demand   VB      O\n",
       "15  Sentence: 1            the   DT      O\n",
       "16  Sentence: 1     withdrawal   NN      O\n",
       "17  Sentence: 1             of   IN      O\n",
       "18  Sentence: 1        British   JJ  B-gpe\n",
       "19  Sentence: 1         troops  NNS      O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We have 6834 sentences which contains 13379 unique words and 17 unique tags\n",
      "And also we have the following distribution of tags\n"
     ]
    }
   ],
   "source": [
    "print(f\" We have {df['Sentence #'].nunique()} sentences which contains {df.Word.nunique()} unique words and {df.Tag.nunique()} unique tags\")\n",
    "print(\"And also we have the following distribution of tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-art</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-eve</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-geo</td>\n",
       "      <td>5130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-gpe</td>\n",
       "      <td>2535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-nat</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-org</td>\n",
       "      <td>2799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-per</td>\n",
       "      <td>2519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-tim</td>\n",
       "      <td>2789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-art</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-eve</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-geo</td>\n",
       "      <td>1077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-gpe</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I-nat</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-org</td>\n",
       "      <td>2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I-per</td>\n",
       "      <td>2709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I-tim</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>O</td>\n",
       "      <td>127031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tag  counts\n",
       "0   B-art      91\n",
       "1   B-eve      80\n",
       "2   B-geo    5130\n",
       "3   B-gpe    2535\n",
       "4   B-nat      46\n",
       "5   B-org    2799\n",
       "6   B-per    2519\n",
       "7   B-tim    2789\n",
       "8   I-art      50\n",
       "9   I-eve      61\n",
       "10  I-geo    1077\n",
       "11  I-gpe      59\n",
       "12  I-nat      21\n",
       "13  I-org    2180\n",
       "14  I-per    2709\n",
       "15  I-tim     823\n",
       "16      O  127031"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Tag').size().reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Now we will use some preprocess for the data and using conditional random fields which is used to parse and label sequencial data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator for getting sentences and groping it into [[Word,Pos tag,Entity tag],[Word,Pos tag,Entity tag],[Word,Pos tag,Entity tag]]\n",
    "# where each value of the sentences list is a sentece formated in this manner\n",
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        :param data:a pandas data frame whit the structure : | Sentence # | Word | POS Tag |\n",
    "            ref: Annotated Corpus for Named Entity Recognition feature Engineered Corpus annotated with IOB and POS tags\n",
    "            https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus#ner_dataset.csv\n",
    "        :attribute n_sent: number of sentence (iterator)\n",
    "        :attribute data: the pandas dataframe fixed in it)\n",
    "        :attribute empty: boolean indicates if there is no data fixed in it\n",
    "        :lambda agg_func: grouping function to structure the sentences for processing\n",
    "        :attribute grouped: a pandas series which contains indexed by sentence number\n",
    "        :attribute empty: boolean indicates if there is no data fixed in it\n",
    "        :attribute sentences: a list of sentences which contains a a list of tagged words in the following format\n",
    "              [[(Word,Pos tag,Entity tag),....,(Word,Pos tag,Entity)],....,[(Word,Pos tag,Entity tag),..,(Word,Pos tag,Entity)]]\n",
    "        \"\"\"\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                           s['POS'].values.tolist(),\n",
    "                                                           s['Tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\"\n",
    "        :return: a sentence iterating one by one\n",
    "        \"\"\"\n",
    "        try:\n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(df)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>Police</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Sentence: 6</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Sentence: 7</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Sentence: 8</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Sentence: 9</td>\n",
       "      <td>Iran</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>Iranian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Sentence: 11</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Sentence: 12</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Sentence: 13</td>\n",
       "      <td>Iran</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Sentence: 14</td>\n",
       "      <td>Two</td>\n",
       "      <td>CD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Sentence: 15</td>\n",
       "      <td>An</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Sentence: 16</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Sentence: 17</td>\n",
       "      <td>Militant</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Sentence: 18</td>\n",
       "      <td>Poor</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Sentence: 19</td>\n",
       "      <td>Suspected</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Sentence: 20</td>\n",
       "      <td>It</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>Sentence: 21</td>\n",
       "      <td>Local</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Sentence: 22</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>Sentence: 23</td>\n",
       "      <td>Iraqi</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Sentence: 24</td>\n",
       "      <td>Officials</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>Sentence: 25</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Sentence: 26</td>\n",
       "      <td>Officials</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>Sentence: 27</td>\n",
       "      <td>Mosul</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Sentence: 28</td>\n",
       "      <td>In</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>Sentence: 29</td>\n",
       "      <td>Egyptian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>Sentence: 30</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149357</th>\n",
       "      <td>Sentence: 6805</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149382</th>\n",
       "      <td>Sentence: 6806</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149413</th>\n",
       "      <td>Sentence: 6807</td>\n",
       "      <td>Meanwhile</td>\n",
       "      <td>RB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149441</th>\n",
       "      <td>Sentence: 6808</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149466</th>\n",
       "      <td>Sentence: 6809</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149484</th>\n",
       "      <td>Sentence: 6810</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149496</th>\n",
       "      <td>Sentence: 6811</td>\n",
       "      <td>A</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149514</th>\n",
       "      <td>Sentence: 6812</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149535</th>\n",
       "      <td>Sentence: 6813</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149552</th>\n",
       "      <td>Sentence: 6814</td>\n",
       "      <td>These</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149585</th>\n",
       "      <td>Sentence: 6815</td>\n",
       "      <td>German</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149606</th>\n",
       "      <td>Sentence: 6816</td>\n",
       "      <td>As</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149640</th>\n",
       "      <td>Sentence: 6817</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149650</th>\n",
       "      <td>Sentence: 6818</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149677</th>\n",
       "      <td>Sentence: 6819</td>\n",
       "      <td>His</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149697</th>\n",
       "      <td>Sentence: 6820</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149710</th>\n",
       "      <td>Sentence: 6821</td>\n",
       "      <td>High-powered</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149746</th>\n",
       "      <td>Sentence: 6822</td>\n",
       "      <td>Abramoff</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149754</th>\n",
       "      <td>Sentence: 6823</td>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149796</th>\n",
       "      <td>Sentence: 6824</td>\n",
       "      <td>As</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149826</th>\n",
       "      <td>Sentence: 6825</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149844</th>\n",
       "      <td>Sentence: 6826</td>\n",
       "      <td>A</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149864</th>\n",
       "      <td>Sentence: 6827</td>\n",
       "      <td>Five</td>\n",
       "      <td>CD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149899</th>\n",
       "      <td>Sentence: 6828</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149915</th>\n",
       "      <td>Sentence: 6829</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149929</th>\n",
       "      <td>Sentence: 6830</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149938</th>\n",
       "      <td>Sentence: 6831</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149961</th>\n",
       "      <td>Sentence: 6832</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149986</th>\n",
       "      <td>Sentence: 6833</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149994</th>\n",
       "      <td>Sentence: 6834</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6834 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sentence #          Word   POS    Tag\n",
       "0          Sentence: 1     Thousands   NNS      O\n",
       "24         Sentence: 2      Families   NNS      O\n",
       "54         Sentence: 3          They   PRP      O\n",
       "68         Sentence: 4        Police   NNS      O\n",
       "83         Sentence: 5           The    DT      O\n",
       "108        Sentence: 6           The    DT      O\n",
       "132        Sentence: 7           The    DT      O\n",
       "153        Sentence: 8           The    DT      O\n",
       "181        Sentence: 9          Iran   NNP  B-gpe\n",
       "196       Sentence: 10       Iranian    JJ  B-gpe\n",
       "221       Sentence: 11           The    DT      O\n",
       "233       Sentence: 12           The    DT      O\n",
       "267       Sentence: 13          Iran   NNP  B-gpe\n",
       "296       Sentence: 14           Two    CD      O\n",
       "322       Sentence: 15            An    DT      O\n",
       "362       Sentence: 16           The    DT      O\n",
       "372       Sentence: 17      Militant    JJ      O\n",
       "394       Sentence: 18          Poor    JJ      O\n",
       "428       Sentence: 19     Suspected    JJ      O\n",
       "448       Sentence: 20            It   PRP      O\n",
       "472       Sentence: 21         Local    JJ      O\n",
       "495       Sentence: 22           The    DT      O\n",
       "524       Sentence: 23         Iraqi    JJ  B-gpe\n",
       "549       Sentence: 24     Officials   NNS      O\n",
       "581       Sentence: 25          U.S.   NNP  B-gpe\n",
       "595       Sentence: 26     Officials   NNS      O\n",
       "618       Sentence: 27         Mosul   NNP  B-geo\n",
       "637       Sentence: 28            In    IN      O\n",
       "656       Sentence: 29      Egyptian    JJ  B-gpe\n",
       "681       Sentence: 30           The    DT      O\n",
       "...                ...           ...   ...    ...\n",
       "149357  Sentence: 6805           The    DT      O\n",
       "149382  Sentence: 6806           The    DT      O\n",
       "149413  Sentence: 6807     Meanwhile    RB      O\n",
       "149441  Sentence: 6808       Denmark   NNP  B-org\n",
       "149466  Sentence: 6809        Muslim   NNP  B-org\n",
       "149484  Sentence: 6810          They   PRP      O\n",
       "149496  Sentence: 6811             A    DT      O\n",
       "149514  Sentence: 6812           The    DT      O\n",
       "149535  Sentence: 6813           The    DT      O\n",
       "149552  Sentence: 6814         These    DT      O\n",
       "149585  Sentence: 6815        German    JJ  B-gpe\n",
       "149606  Sentence: 6816            As    IN      O\n",
       "149640  Sentence: 6817           Mr.   NNP  B-per\n",
       "149650  Sentence: 6818           The    DT      O\n",
       "149677  Sentence: 6819           His  PRP$      O\n",
       "149697  Sentence: 6820           Mr.   NNP  B-per\n",
       "149710  Sentence: 6821  High-powered    JJ      O\n",
       "149746  Sentence: 6822      Abramoff   NNP  B-per\n",
       "149754  Sentence: 6823            He   PRP      O\n",
       "149796  Sentence: 6824            As    IN      O\n",
       "149826  Sentence: 6825           The    DT  B-org\n",
       "149844  Sentence: 6826             A    DT      O\n",
       "149864  Sentence: 6827          Five    CD      O\n",
       "149899  Sentence: 6828           The    DT      O\n",
       "149915  Sentence: 6829           The    DT      O\n",
       "149929  Sentence: 6830       Chinese    JJ  B-gpe\n",
       "149938  Sentence: 6831           The    DT      O\n",
       "149961  Sentence: 6832       Beijing   NNP  B-geo\n",
       "149986  Sentence: 6833       Chinese    JJ  B-gpe\n",
       "149994  Sentence: 6834       Beijing   NNP  B-geo\n",
       "\n",
       "[6834 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Sentence #').head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### A little explanation:\n",
    "  we are grouping our data by sentences in such a way that we will construct a list of sentences, which contains a list of its' words where the words have its' POS\n",
    "  tag and its' entity tag in this manner\n",
    "  \n",
    "  [[(Word,Pos tag,Entity tag),....,(Word,Pos tag,Entity)],....,[(Word,Pos tag,Entity tag),..,(Word,Pos tag,Entity)]]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Thousands', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('demonstrators', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('marched', 'VBN', 'O'),\n",
       "  ('through', 'IN', 'O'),\n",
       "  ('London', 'NNP', 'B-geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('protest', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('war', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Iraq', 'NNP', 'B-geo'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('demand', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('withdrawal', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('British', 'JJ', 'B-gpe'),\n",
       "  ('troops', 'NNS', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('that', 'DT', 'O'),\n",
       "  ('country', 'NN', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Iranian', 'JJ', 'B-gpe'),\n",
       "  ('officials', 'NNS', 'O'),\n",
       "  ('say', 'VBP', 'O'),\n",
       "  ('they', 'PRP', 'O'),\n",
       "  ('expect', 'VBP', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('get', 'VB', 'O'),\n",
       "  ('access', 'NN', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('sealed', 'JJ', 'O'),\n",
       "  ('sensitive', 'JJ', 'O'),\n",
       "  ('parts', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('plant', 'NN', 'O'),\n",
       "  ('Wednesday', 'NNP', 'B-tim'),\n",
       "  (',', ',', 'O'),\n",
       "  ('after', 'IN', 'O'),\n",
       "  ('an', 'DT', 'O'),\n",
       "  ('IAEA', 'NNP', 'B-org'),\n",
       "  ('surveillance', 'NN', 'O'),\n",
       "  ('system', 'NN', 'O'),\n",
       "  ('begins', 'VBZ', 'O'),\n",
       "  ('functioning', 'VBG', 'O'),\n",
       "  ('.', '.', 'O')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Then we will extract  more  features and transform our data in  format defined by the sklearn crf suite wrapper\n",
    "https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \"\"\"\n",
    "    :param sent: a sentence in the format  [[(Word,Pos tag,Entity tag),....,(Word,Pos tag,Entity)],....,[(Word,Pos tag,Entity tag),..,(Word,Pos tag,Entity)]]\n",
    "    :param i: position of the word desired to extract its features\n",
    "    :return: a dictionary of features in  the sklearn CRF wrapper defined format\n",
    "    \"\"\"\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        #weight of the word\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        #postag and type\n",
    "        'postag': postag,\n",
    "        #simple postag\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        #previous word in the sentence info (if is not the first word in sentence)\n",
    "        word1 = sent[i - 1][0]\n",
    "        postag1 = sent[i - 1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent) - 1:\n",
    "        #next word info (if is not the last word in sentence)\n",
    "        word1 = sent[i + 1][0]\n",
    "        postag1 = sent[i + 1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        #if is the End Of the Sentence\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\"\n",
    "    :param sent: a sentence to obtain its features\n",
    "    :return: an array of features per each word  in the sentence\n",
    "    \"\"\"\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    \"\"\" \n",
    "    :param sent: a sentence to obtain the labels present in \n",
    "    :return: an aray of labels present in the sentences (entities)\n",
    "    \"\"\"\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    \"\"\"\n",
    "    :param sent:  a sentence to obtain the tokens  in \n",
    "    :return: an array of tokens of the sentences (words)\n",
    "    \"\"\"\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Then we will build our train and test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets visualize how our data is arranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bias': 1.0, 'word.lower()': 'philip', 'word[-3:]': 'lip', 'word[-2:]': 'ip', 'word.isupper()': False, 'word.isdigit()': False, 'postag': 'NNP', 'postag[:2]': 'NN', 'BOS': True, '+1:word.lower()': 'alston', '+1:word.isupper()': False, '+1:postag': 'NNP', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'alston', 'word[-3:]': 'ton', 'word[-2:]': 'on', 'word.isupper()': False, 'word.isdigit()': False, 'postag': 'NNP', 'postag[:2]': 'NN', '-1:word.lower()': 'philip', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'NNP', '-1:postag[:2]': 'NN', '+1:word.lower()': 'said', '+1:word.isupper()': False, '+1:postag': 'VBD', '+1:postag[:2]': 'VB'}, {'bias': 1.0, 'word.lower()': 'said', 'word[-3:]': 'aid', 'word[-2:]': 'id', 'word.isupper()': False, 'word.isdigit()': False, 'postag': 'VBD', 'postag[:2]': 'VB', '-1:word.lower()': 'alston', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'NNP', '-1:postag[:2]': 'NN', '+1:word.lower()': 'he', '+1:word.isupper()': False, '+1:postag': 'PRP', '+1:postag[:2]': 'PR'}, {'bias': 1.0, 'word.lower()': 'he', 'word[-3:]': 'he', 'word[-2:]': 'he', 'word.isupper()': False, 'word.isdigit()': False, 'postag': 'PRP', 'postag[:2]': 'PR', '-1:word.lower()': 'said', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'VBD', '-1:postag[:2]': 'VB', '+1:word.lower()': 'is', '+1:word.isupper()': False, '+1:postag': 'VBZ', '+1:postag[:2]': 'VB'}, {'bias': 1.0, 'word.lower()': 'is', 'word[-3:]': 'is', 'word[-2:]': 'is', 'word.isupper()': False, 'word.isdigit()': False, 'postag': 'VBZ', 'postag[:2]': 'VB', '-1:word.lower()': 'he', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PRP', '-1:postag[:2]': 'PR', '+1:word.lower()': 'deeply', '+1:word.isupper()': False, '+1:postag': 'RB', '+1:postag[:2]': 'RB'}] \n",
      "\n",
      "\n",
      " ['B-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][:5],\"\\n\\n\\n\",y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we will train our model , in this case we will use conditional random fields which allow us to take care of the sequence in the data \n",
    "\n",
    "we will use the Limited memory BFGS optimization algorithm because of the limited memory for training the model,this is an alternative to gradient descent algorithm and it's also to minimize the error \n",
    "\n",
    "https://en.wikipedia.org/wiki/Limited-memory_BFGS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and evaluating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tags=np.unique(df[\"Tag\"]).tolist()\n",
    "Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now because O (non entity) is our most common tag it will make our results looks better than they are , so when we'll measure the quality of the model we will avoid O value so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tags.pop()\n",
    "Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.33      0.12      0.18        16\n",
      "       B-eve       0.78      0.41      0.54        17\n",
      "       B-geo       0.79      0.90      0.84      1047\n",
      "       B-gpe       0.92      0.78      0.85       509\n",
      "       B-nat       0.67      0.44      0.53         9\n",
      "       B-org       0.74      0.66      0.70       580\n",
      "       B-per       0.82      0.82      0.82       501\n",
      "       B-tim       0.91      0.83      0.87       562\n",
      "       I-art       0.00      0.00      0.00         7\n",
      "       I-eve       0.89      0.62      0.73        13\n",
      "       I-geo       0.78      0.83      0.80       197\n",
      "       I-gpe       1.00      0.36      0.53        11\n",
      "       I-nat       0.75      1.00      0.86         3\n",
      "       I-org       0.76      0.71      0.74       463\n",
      "       I-per       0.79      0.89      0.83       543\n",
      "       I-tim       0.84      0.72      0.77       170\n",
      "\n",
      "   micro avg       0.81      0.80      0.81      4648\n",
      "   macro avg       0.74      0.63      0.66      4648\n",
      "weighted avg       0.81      0.80      0.80      4648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(y_test, y_pred, labels = Tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing a use case \n",
      " Counter({'O': 16, 'B-gpe': 1, 'B-tim': 1}) \n",
      " Counter({'O': 16, 'B-gpe': 1, 'B-tim': 1})\n"
     ]
    }
   ],
   "source": [
    "countert=Counter(y_test[1])\n",
    "counterp=Counter(y_pred[1])\n",
    "print(\"Testing a use case \\n\",countert,\"\\n\",counterp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus on the main objective: \"identify ML entities \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just for making a look we will check how our current training is working in a real enviroment ( ML text) \n",
    "\n",
    "So we have the following text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Named-entity recognition (NER) refers to a data extraction task that is responsible for finding, storing and sorting textual content into default categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values and percentages.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to apply the same ***preprocessing*** we do in our training data in order to use our model\n",
    "\n",
    "Then we need to separate the text in sentences and also POS tag the sentences, we will use spacy library to do that and also compare results in NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_md==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz#egg=en_core_web_md==2.0.0 in /anaconda3/envs/mlprojectsy/lib/python3.6/site-packages (2.0.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/envs/mlprojectsy/lib/python3.6/site-packages/en_core_web_md\n",
      "    -->\n",
      "    /anaconda3/envs/mlprojectsy/lib/python3.6/site-packages/spacy/data/en_core_web_md\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_md')\n",
      "\n",
      "Our doc is:\n",
      "Named-entity recognition (NER) refers to a data extraction task that is responsible for finding, storing and sorting textual content into default categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values and percentages.\n",
      "\n",
      "Its conformed by the sentences\n",
      "\n",
      "[Named-entity recognition (NER) refers to a data extraction task that is responsible for finding, storing and sorting textual content into default categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values and percentages.]\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(text)\n",
    "print(f\"Our doc is:\\n{doc}\\n\\nIts conformed by the sentences\\n\\n{list(doc.sents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named-entity recognition (NER) refers to a data extraction task that is responsible for finding, storing and sorting textual content into default categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values and percentages. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Picking up a sentence from the text.\n",
    "sentence=doc.sents.__next__()\n",
    "ent=sentence[0]\n",
    "print(sentence,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data\n",
    "translation=str.maketrans('','','-')\n",
    "#we will remove the - character from the spacy pos tags because in our train set we havent tags whit that character\n",
    "processedSentence=[(w.text,w.tag_.translate(translation)) for w in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Named', 'VBN'),\n",
       " ('-', 'HYPH'),\n",
       " ('entity', 'NN'),\n",
       " ('recognition', 'NN'),\n",
       " ('(', 'LRB'),\n",
       " ('NER', 'NNP'),\n",
       " (')', 'RRB'),\n",
       " ('refers', 'VBZ'),\n",
       " ('to', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('data', 'NN'),\n",
       " ('extraction', 'NN'),\n",
       " ('task', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('responsible', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('finding', 'VBG'),\n",
       " (',', ','),\n",
       " ('storing', 'VBG'),\n",
       " ('and', 'CC'),\n",
       " ('sorting', 'VBG'),\n",
       " ('textual', 'JJ'),\n",
       " ('content', 'NN'),\n",
       " ('into', 'IN'),\n",
       " ('default', 'NN'),\n",
       " ('categories', 'NNS'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('names', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('persons', 'NNS'),\n",
       " (',', ','),\n",
       " ('organizations', 'NNS'),\n",
       " (',', ','),\n",
       " ('locations', 'NNS'),\n",
       " (',', ','),\n",
       " ('expressions', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('times', 'NNS'),\n",
       " (',', ','),\n",
       " ('quantities', 'NNS'),\n",
       " (',', ','),\n",
       " ('monetary', 'JJ'),\n",
       " ('values', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('percentages', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's extract features and predict the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceFeatures=sent2features(processedSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Named-entity recognition (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    NER\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") refers to a data extraction task that is responsible for finding, storing and sorting textual content into default categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values and percentages.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction=crf.predict([sentenceFeatures])\n",
    "\n",
    "from tabulate import tabulate\n",
    "from spacy import displacy\n",
    "displacy.render(sentence, style='ent', jupyter=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word:('Named', 'VBN') | Prediction:O\n",
      "\n",
      "Word:('-', 'HYPH') | Prediction:O\n",
      "\n",
      "Word:('entity', 'NN') | Prediction:O\n",
      "\n",
      "Word:('recognition', 'NN') | Prediction:O\n",
      "\n",
      "Word:('(', 'LRB') | Prediction:O\n",
      "\n",
      "Word:('NER', 'NNP') | Prediction:B-org\n",
      "\n",
      "Word:(')', 'RRB') | Prediction:O\n",
      "\n",
      "Word:('refers', 'VBZ') | Prediction:O\n",
      "\n",
      "Word:('to', 'IN') | Prediction:O\n",
      "\n",
      "Word:('a', 'DT') | Prediction:O\n",
      "\n",
      "Word:('data', 'NN') | Prediction:O\n",
      "\n",
      "Word:('extraction', 'NN') | Prediction:O\n",
      "\n",
      "Word:('task', 'NN') | Prediction:O\n",
      "\n",
      "Word:('that', 'WDT') | Prediction:O\n",
      "\n",
      "Word:('is', 'VBZ') | Prediction:O\n",
      "\n",
      "Word:('responsible', 'JJ') | Prediction:O\n",
      "\n",
      "Word:('for', 'IN') | Prediction:O\n",
      "\n",
      "Word:('finding', 'VBG') | Prediction:O\n",
      "\n",
      "Word:(',', ',') | Prediction:O\n",
      "\n",
      "Word:('storing', 'VBG') | Prediction:O\n",
      "\n",
      "Word:('and', 'CC') | Prediction:O\n",
      "\n",
      "Word:('sorting', 'VBG') | Prediction:O\n",
      "\n",
      "Word:('textual', 'JJ') | Prediction:O\n",
      "\n",
      "Word:('content', 'NN') | Prediction:O\n",
      "\n",
      "Word:('into', 'IN') | Prediction:O\n",
      "\n",
      "Word:('default', 'NN') | Prediction:O\n",
      "\n",
      "Word:('categories', 'NNS') | Prediction:O\n",
      "\n",
      "Word:('such', 'JJ') | Prediction:O\n",
      "\n",
      "Word:('as', 'IN') | Prediction:O\n",
      "\n",
      "Word:('the', 'DT') | Prediction:O\n",
      "\n",
      "Word:('names', 'NNS') | Prediction:O\n",
      "\n",
      "Word:('of', 'IN') | Prediction:O\n",
      "\n",
      "Word:('persons', 'NNS') | Prediction:O\n",
      "\n",
      "Word:(',', ',') | Prediction:O\n",
      "\n",
      "Word:('organizations', 'NNS') | Prediction:O\n",
      "\n",
      "Word:(',', ',') | Prediction:O\n",
      "\n",
      "Word:('locations', 'NNS') | Prediction:O\n",
      "\n",
      "Word:(',', ',') | Prediction:O\n",
      "\n",
      "Word:('expressions', 'NNS') | Prediction:O\n",
      "\n",
      "Word:('of', 'IN') | Prediction:O\n",
      "\n",
      "Word:('times', 'NNS') | Prediction:O\n",
      "\n",
      "Word:(',', ',') | Prediction:O\n",
      "\n",
      "Word:('quantities', 'NNS') | Prediction:O\n",
      "\n",
      "Word:(',', ',') | Prediction:O\n",
      "\n",
      "Word:('monetary', 'JJ') | Prediction:O\n",
      "\n",
      "Word:('values', 'NNS') | Prediction:O\n",
      "\n",
      "Word:('and', 'CC') | Prediction:O\n",
      "\n",
      "Word:('percentages', 'NNS') | Prediction:O\n",
      "\n",
      "Word:('.', '.') | Prediction:O\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prediction[0])):\n",
    "    print(f\"\\nWord:{processedSentence[i]} | Prediction:{prediction[0][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As  we can see our predictions are quite accurate, so we also propose the next steps to improve our prediction\n",
    "- Do some data mining and create a small dataset including ML text sentences\n",
    "- Then Including in this custom dataset a custom \"POS TAG\" named ML. This could help us to know where ML learning entities use to appear in a ML text \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our ML custom data set\n",
    "\n",
    "We want to build a custom dataset and also a helper to extract data from machine learning pdfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in /anaconda3/envs/mlProjectsy/lib/python3.6/site-packages (20170720)\n",
      "Requirement already satisfied: chardet in /anaconda3/envs/mlProjectsy/lib/python3.6/site-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: pycryptodome in /anaconda3/envs/mlProjectsy/lib/python3.6/site-packages (from pdfminer.six) (3.6.6)\n",
      "Requirement already satisfied: six in /anaconda3/envs/mlProjectsy/lib/python3.6/site-packages (from pdfminer.six) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter#process_pdf\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdfname):\n",
    "    # PDFMiner boilerplate\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    sio = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, sio, codec=codec, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # Extract text\n",
    "    fp = open(pdfname, 'rb')\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "\n",
    "    # Get text from StringIO\n",
    "    text = sio.getvalue()\n",
    "\n",
    "    # Cleanup\n",
    "    device.close()\n",
    "    sio.close()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pdf_to_text(\"mltop10.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will build a new data set using a ml post from medium.com and try to get sentences that contains important and common machine learning entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=(list(doc.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les make a look in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Of course, the algorithms you try must be appropriate for your problem,\n",
      "which is where picking the right machine learning task comes in., As an\n",
      "analogy, if you need to clean your house, you might use a vacuum, a\n",
      "broom, or a mop, but you wouldn’t bust out a shovel and start digging.\n",
      "\n",
      ", The Big Principle\n",
      "\n",
      ", However, there is a common principle that underlies all supervised\n",
      "machine learning algorithms for predictive modeling.\n",
      "\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[10:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the spacy entity recognition module to identify some ML entities and then we will defined for getting an improved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Newbies, James Le Jan 20, \n",
      "\n",
      ", the “, No Free Lunch, \n",
      "\n",
      ", \n",
      "\n",
      ", \n",
      "\n",
      ", \n",
      "\n",
      ", \n",
      "\n",
      ", \n",
      "\n",
      ", Y, \n",
      "\n",
      ", \n",
      "\n",
      ", Y =, Y, \n",
      "\n",
      ", 10, Linear Regression, one, \n",
      "\n",
      ", \n",
      "\n",
      ", B1, B0, B1, \n",
      "\n",
      ", more than 200 years, two, \n",
      "\n",
      ", \n",
      "\n",
      ", 0, 0 and 1, less than 0.5, \n",
      "\n",
      ", 3, Linear Discriminant Analysis, two, more than two, Linear Discriminant Analysis, \n",
      "\n",
      ", \n",
      "\n",
      "1, 2, \n",
      "\n",
      ", Linear Discriminant Analysis, Gaussian, \n",
      "\n",
      ", 4, \n",
      "\n",
      ", 5, \n",
      "\n",
      ", two, 1, 2, Bayes Theorem, Gaussian, 6, KNN, KNN, \n",
      "\n",
      ", \n",
      "\n",
      ", \n",
      "inches, \n",
      "\n",
      ", \n",
      "\n",
      ", \n",
      "\n",
      ", 7, K-Nearest Neighbors, The Learning Vector Quantization, \n",
      "\n",
      ", LVQ, K-Nearest Neighbors, between 0 and 1, \n",
      "\n",
      ", KNN, LVQ, \n",
      "\n",
      ", 8, Vector Machines, one, \n",
      "\n",
      ", SVM, two-dimensions, SVM, \n",
      "\n",
      ", two, \n",
      "\n",
      ", one, \n",
      "\n",
      ", 9, Random Forest, Random Forest, one, \n",
      "\n",
      ", \n",
      "\n",
      ", Random Forest, \n",
      "\n",
      ", \n",
      "\n",
      ", \n",
      "\n",
      ", 10, second, first, first, AdaBoost, \n",
      "\n",
      ", first, \n",
      "\n",
      ", \n",
      "\n",
      ", 1, 2, 3, 4, \n",
      "\n",
      ", Machine Learning, Machine Learning, \n",
      "\n",
      ", — —\n",
      "\n",
      ", GitHub, https://jameskle.com/., LinkedIn, \n",
      "\n",
      " )\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are many ML entities.\n",
    "we have selected the sentences that contains that ML entities \n",
    "and that are enough larger to contribute  significant feaures for our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "definedMLentites=[\"KNN\",\"Random\",\"Forest\",\"SVM\",\"K-Nearest\",\"Neighbors\",\"Regression\",\"Trees\",\"Linear\",\"Regression\",\"LVQ\",\"Naive\",\"Bayes\",\"Linear\",\"Discriminant\",\"Analysis\",\"LDA\",\"Random\",\"Forest\",\"Neural\",\"Network\",\"Logistic\",\"Regression\"]\n",
    "lowerDML=[mlen.lower() for mlen in definedMLentites]\n",
    "customData=[sentence for sentence in doc.sents if len(sentence)>9 and  any(mlEnt in sentence.text for mlEnt in definedMLentites)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear regression is perhaps one of the most well-known and well-,\n",
       " Linear regression has been around for more than 200 years and has been\n",
       " extensively studied.,\n",
       " Logistic regression is another technique borrowed by machine learning\n",
       " from the field of statistics.,\n",
       " Logistic regression is like linear regression in that the goal is to find the\n",
       " values for the coefficients that weight each input variable.]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customData[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we can see we have a good selection of sentences that contain the most common ML entities so we will extract these sentences and tag them using a custom IOB TAG named \"I/O-ML\" that will reference that we are talking about machine learning\n",
    "\n",
    "Now we will POS tag the dataset and do some entity recognition, after we will change the tags to indicate where ML entities are located "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Linear', 'NNP', 'B-ML'),\n",
       "  ('regression', 'NN', 'I-ML'),\n",
       "  ('is', 'VBZ', 'O'),\n",
       "  ('perhaps', 'RB', 'O'),\n",
       "  ('one', 'CD', 'B-cardinal'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('most', 'RBS', 'O'),\n",
       "  ('well', 'RB', 'O'),\n",
       "  ('-', 'HYPH', 'O'),\n",
       "  ('known', 'VBN', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('well-', 'JJ', 'O'),\n",
       "  ('\\n', '', 'O')],\n",
       " [('Linear', 'NNP', 'B-ML'),\n",
       "  ('regression', 'NN', 'I-ML'),\n",
       "  ('has', 'VBZ', 'O'),\n",
       "  ('been', 'VBN', 'O'),\n",
       "  ('around', 'RB', 'O'),\n",
       "  ('for', 'IN', 'O'),\n",
       "  ('more', 'JJR', 'B-date'),\n",
       "  ('than', 'IN', 'I-date'),\n",
       "  ('200', 'CD', 'I-date'),\n",
       "  ('years', 'NNS', 'I-date'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('has', 'VBZ', 'O'),\n",
       "  ('been', 'VBN', 'O'),\n",
       "  ('\\n', '', 'O'),\n",
       "  ('extensively', 'RB', 'O'),\n",
       "  ('studied', 'VBD', 'O'),\n",
       "  ('.', '.', 'O')]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customData=[[(word.text,word.tag_.translate(translation),\"B-ML\" if word.text in definedMLentites else \"I-ML\" if word.text in lowerDML else \"O\" if word.ent_iob_ == \"O\"  else f\"{word.ent_iob_}-{word.ent_type_.lower()}\") for word in sentence] for sentence in customData]\n",
    "customData[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add this custom data to our training data and see what happens\n",
    "\n",
    "First we have to extract the features\n",
    "### Building the training and testing set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qq=[]testing\n",
    "#qq.extend(list(map(sent2features,customData)))\n",
    "X.extend([sent2features(s) for s in customData])\n",
    "y.extend([sent2labels(s) for s in customData])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test our model using some ml texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"To solve a classification task by a supervised machine learning model like SVM, the task usually involves with training and testing data, which consist of some data instances.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we have to preprocess the ml text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nlp(test)\n",
    "#Picking up a sentence from the text.\n",
    "sentence=test.sents.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data\n",
    "translation=str.maketrans('','','-')\n",
    "#we will remove the - character from the spacy pos tags because in our train set we havent tags whit that character\n",
    "processedSentence=[(w.text,w.tag_.translate(translation)) for w in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To', 'TO'),\n",
       " ('solve', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('classification', 'NN'),\n",
       " ('task', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('supervised', 'JJ'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('model', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('SVM', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('task', 'NN'),\n",
       " ('usually', 'RB'),\n",
       " ('involves', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('training', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('testing', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('consist', 'VBP'),\n",
       " ('of', 'IN'),\n",
       " ('some', 'DT'),\n",
       " ('data', 'NN'),\n",
       " ('instances', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceFeatures=sent2features(processedSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make a prediction using spacy and then comparing the prediction using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">To solve a classification task by a supervised machine learning model like \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    SVM\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", the task usually involves with training and testing data, which consist of some data instances.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy \n",
    "displacy.render(sentence, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=crf.predict([sentenceFeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word:('To', 'TO') | Prediction:O\n",
      "\n",
      "Word:('solve', 'VB') | Prediction:O\n",
      "\n",
      "Word:('a', 'DT') | Prediction:O\n",
      "\n",
      "Word:('classification', 'NN') | Prediction:O\n",
      "\n",
      "Word:('task', 'NN') | Prediction:O\n",
      "\n",
      "Word:('by', 'IN') | Prediction:O\n",
      "\n",
      "Word:('a', 'DT') | Prediction:O\n",
      "\n",
      "Word:('supervised', 'JJ') | Prediction:O\n",
      "\n",
      "Word:('machine', 'NN') | Prediction:O\n",
      "\n",
      "Word:('learning', 'VBG') | Prediction:O\n",
      "\n",
      "Word:('model', 'NN') | Prediction:O\n",
      "\n",
      "Word:('like', 'IN') | Prediction:O\n",
      "\n",
      "Word:('SVM', 'NNP') | Prediction:B-org\n",
      "\n",
      "Word:(',', ',') | Prediction:O\n",
      "\n",
      "Word:('the', 'DT') | Prediction:O\n",
      "\n",
      "Word:('task', 'NN') | Prediction:O\n",
      "\n",
      "Word:('usually', 'RB') | Prediction:O\n",
      "\n",
      "Word:('involves', 'VBZ') | Prediction:O\n",
      "\n",
      "Word:('with', 'IN') | Prediction:O\n",
      "\n",
      "Word:('training', 'NN') | Prediction:O\n",
      "\n",
      "Word:('and', 'CC') | Prediction:O\n",
      "\n",
      "Word:('testing', 'NN') | Prediction:O\n",
      "\n",
      "Word:('data', 'NNS') | Prediction:O\n",
      "\n",
      "Word:(',', ',') | Prediction:O\n",
      "\n",
      "Word:('which', 'WDT') | Prediction:O\n",
      "\n",
      "Word:('consist', 'VBP') | Prediction:O\n",
      "\n",
      "Word:('of', 'IN') | Prediction:O\n",
      "\n",
      "Word:('some', 'DT') | Prediction:O\n",
      "\n",
      "Word:('data', 'NN') | Prediction:O\n",
      "\n",
      "Word:('instances', 'NNS') | Prediction:O\n",
      "\n",
      "Word:('.', '.') | Prediction:O\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prediction[0])):\n",
    "    print(f\"\\nWord:{processedSentence[i]} | Prediction:{prediction[0][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model correctly predics that SVM is a ML entity \n",
    "\n",
    "Now we will meeasure the accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=crf.predict(X_test)\n",
    "Tags.extend([\"I-ML\",\"B-ML\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.40      0.14      0.21        14\n",
      "       B-eve       0.78      0.44      0.56        16\n",
      "       B-geo       0.81      0.87      0.84      1060\n",
      "       B-gpe       0.92      0.80      0.86       484\n",
      "       B-nat       0.83      0.45      0.59        11\n",
      "       B-org       0.73      0.70      0.71       581\n",
      "       B-per       0.81      0.84      0.83       499\n",
      "       B-tim       0.91      0.85      0.88       565\n",
      "       I-art       0.00      0.00      0.00         3\n",
      "       I-eve       0.86      0.40      0.55        15\n",
      "       I-geo       0.82      0.73      0.77       219\n",
      "       I-gpe       1.00      0.25      0.40        12\n",
      "       I-nat       0.75      1.00      0.86         3\n",
      "       I-org       0.76      0.73      0.75       497\n",
      "       I-per       0.81      0.90      0.86       575\n",
      "       I-tim       0.79      0.70      0.75       162\n",
      "        I-ML       0.00      0.00      0.00         2\n",
      "        B-ML       0.00      0.00      0.00         5\n",
      "\n",
      "   micro avg       0.82      0.80      0.81      4723\n",
      "   macro avg       0.67      0.54      0.58      4723\n",
      "weighted avg       0.82      0.80      0.81      4723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/mlProjectsy/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(y_test, y_pred, labels = Tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see: When we perfom the measure we got some 0 values for the ML Tags but this only happens because of the big difference of data that we have (150000) vs 24 from the custom dataset. As a result we can say that our model predicts ML entities correctly, but could be further improved, if we would have sufficient data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use spacy for build a custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new entity label\n",
    "LABEL = 'ML'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now spacy uses it owns training data format so  in the following format\n",
    "\n",
    "TRAIN_DATA = [(\"Phrase\", {'entities': [(0, 6, 'ANIMAL')]}),(\"Phrase\", {'entities': []})]\n",
    "\n",
    "So lets us the data that we have obtain previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "definedMLentites=[\"KNN\",\"Random\",\"Forest\",\"SVM\",\"K-Nearest\",\"Neighbors\",\"Regression\",\"Trees\",\"Linear\",\"Regression\",\"LVQ\",\"Naive Bayes\",\"Linear\",\"Discriminant\",\"Analysis\",\"LDA\",\"Random\",\"Forest\",\"Neural\",\"Network\",\"Logistic\",\"Regression\"]\n",
    "lowerDML=[mlen.lower() for mlen in definedMLentites]\n",
    "customData=[sentence for sentence in doc.sents if len(sentence)>9 and  any(mlEnt in sentence.text for mlEnt in definedMLentites)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear regression is perhaps one of the most well-known and well-,\n",
       " Linear regression has been around for more than 200 years and has been\n",
       " extensively studied.,\n",
       " Logistic regression is another technique borrowed by machine learning\n",
       " from the field of statistics.,\n",
       " Logistic regression is like linear regression in that the goal is to find the\n",
       " values for the coefficients that weight each input variable.,\n",
       " Logistic Regression is a classification algorithm traditionally limited to\n",
       " only two-class classification problems.]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customData[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Linear regression is perhaps one of the most well-known and well-\\n', {'entities': [(0, 17, 'ML')]}), ('Linear regression has been around for more than 200 years and has been\\nextensively studied.', {'entities': [(0, 17, 'ML')]}), ('Logistic regression is another technique borrowed by machine learning\\nfrom the field of statistics.', {'entities': [(0, 19, 'ML')]}), ('Logistic regression is like linear regression in that the goal is to find the\\nvalues for the coefficients that weight each input variable.', {'entities': [(28, 45, 'ML'), (0, 19, 'ML')]}), ('Logistic Regression is a classification algorithm traditionally limited to\\nonly two-class classification problems.', {'entities': [(0, 19, 'ML')]})]\n"
     ]
    }
   ],
   "source": [
    "definedMLentites=[\"KNN\",\"Random Forest\",\"SVM\",\"K-Nearest Neighbors\",\"Decision Trees\",\"Trees\",\"Linear Regression\",\"LVQ\",\"Naive Bayes\",\"Linear Discriminant Analysis\",\"Learning Vector Quantization\",\"LDA\",\"Random Forest\",\"Neural Network\",\"Logistic Regression\"]\n",
    "\n",
    "#TRAIN_DATA = [(sentence, {'entities': [(0, 6, 'ANIMAL')]\n",
    "import re\n",
    "\n",
    "TRAIN_DATA=list()\n",
    "\n",
    "regexEnt=[re.compile(ent, re.IGNORECASE) for ent in definedMLentites]\n",
    "for sentence in customData:\n",
    "    matches=[]\n",
    "    for regex in regexEnt:\n",
    "        for match in regex.finditer(sentence.text):\n",
    "            matches.append((match.start(),match.end(),\"ML\"))\n",
    "    TRAIN_DATA.append((sentence.text,{\"entities\":matches}))\n",
    "\n",
    "print(TRAIN_DATA[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_name=\"ML\"\n",
    "output_dir=\"SpacyModels\"\n",
    "def trainSpacyNER(model=None,new_model_name='ML', output_dir=\"SPacy\", n_iter=10):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print('Losses', losses)\n",
    "    # save model to output directory\n",
    "    print(output_dir)\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "    return nlp\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "Losses {'ner': 14.03367133517014}\n",
      "Losses {'ner': 4.3819994722815485}\n",
      "Losses {'ner': 3.863196328151602}\n",
      "Losses {'ner': 2.616340077769049}\n",
      "Losses {'ner': 1.7884353150897003}\n",
      "Losses {'ner': 1.0731156938768376}\n",
      "Losses {'ner': 1.3629447333280211}\n",
      "Losses {'ner': 0.5518898429205578}\n",
      "Losses {'ner': 0.6829992751232408}\n",
      "Losses {'ner': 0.4944052013255248}\n",
      "SpacyModels\n",
      "Saved model to SpacyModels\n"
     ]
    }
   ],
   "source": [
    "nlp=trainSpacyNER(None,new_model_name=new_model_name,output_dir=\"SpacyModels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the prediction and visualizing  the resuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">A \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Support Vector Machine\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ML</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    SVM\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ML</span>\n",
       "</mark>\n",
       ") is a discriminative classifier formally defined by a separating hyperplane.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the trained model\n",
    "test_text = \"\"\"A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane.\"\"\"\n",
    "doc = nlp(test_text)\n",
    "#Lets vizualize the entities in the test_text\n",
    "colors = {'ML': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)'}\n",
    "options = {'ents': ['ML'], 'colors': colors}\n",
    "displacy.render(doc, style='ent',jupyter=True,options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlprojectsy)",
   "language": "python",
   "name": "mlprojectsy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
